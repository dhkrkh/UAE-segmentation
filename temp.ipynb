{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import*\nfrom tensorflow.keras.layers import*\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import*\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.image import ResizeMethod\nimport albumentations as A\n\nimport glob\nimport tifffile as tiff\nimport cv2\nfrom skimage.io import imsave, imread, imshow\nfrom skimage import color\nfrom PIL import Image\nfrom skimage.transform import resize\nimport os\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom tensorflow.keras.metrics import MeanIoU, Accuracy\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.resnet50 import preprocess_input",
      "metadata": {
        "id": "wKgHnPDfVtFU",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:34:45.195897Z",
          "iopub.execute_input": "2022-05-31T00:34:45.196242Z",
          "iopub.status.idle": "2022-05-31T00:34:52.956570Z",
          "shell.execute_reply.started": "2022-05-31T00:34:45.196213Z",
          "shell.execute_reply": "2022-05-31T00:34:52.955792Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-05-31T00:34:52.957971Z",
          "iopub.execute_input": "2022-05-31T00:34:52.958588Z",
          "iopub.status.idle": "2022-05-31T00:34:52.962235Z",
          "shell.execute_reply.started": "2022-05-31T00:34:52.958560Z",
          "shell.execute_reply": "2022-05-31T00:34:52.961561Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "color_dict = {\n    0: [60,16,152], # Building\n    1: [132,41,246], # Land\n    2: [110,193,228], # road\n    3: [254,221,58], # Vegetation\n    4: [226,169,41], # water\n    5: [155,155,155], # Unlabeled\n    # 6: [0,0,0] # Padding\n}\n\ndef rgb_to_onehot(rgb_arr, color_dict):\n    num_classes = len(color_dict)\n    shape = rgb_arr.shape[:2] + (num_classes,)\n    arr = np.zeros(shape,dtype=np.int8)\n\n    for i, cls in enumerate(color_dict):\n        arr[:,:,i] = np.all(rgb_arr.reshape((-1,3))==color_dict[i],axis=1).reshape(shape[:2])\n        arr = np.array(arr)\n    return arr\n\ndef onehot_to_rgb(onehot, color_dict):\n    output = np.zeros(onehot.shape[:2] + (3,))\n    mask = np.argmax(onehot,axis=2)\n    for k in color_dict.keys():\n        output[mask==k] = color_dict[k]\n    return output.astype(int)\n\ndef get_dataset(X_dir, y_dir, color_dict, flag):\n    num_samples = len(X_dir)\n    X = []\n    y = []\n\n    for i in range(num_samples):\n        image = plt.imread(X_dir[i])\n        mask = plt.imread(y_dir[i])\n\n        X.append(np.array(tf.image.resize_with_crop_or_pad(image, 512, 512))) \n        y.append(rgb_to_onehot((np.array(tf.image.resize_with_crop_or_pad(mask[:,:,0:3], 512, 512)))*255, color_dict)) \n        # if size is large -> centrally crop\n        # if size is less -> centrally pad with zeros\n\n        if flag == 0:\n            for j in range(8): # creating 8 augmented copies for each train image\n                transform = augment(512, 512)\n                transformed = transform(image=image, mask=mask)\n                transformed_image = transformed['image']\n                transformed_mask = transformed['mask']\n\n                X.append(transformed_image)\n                y.append(rgb_to_onehot((transformed_mask[:,:,0:3])*255, color_dict))\n\n    return np.array(X), np.array(y)\n\ndef augment(width, height):\n    transform = A.Compose([\n      A.RandomCrop(width=width, height=height, p=1.0),\n      A.HorizontalFlip(p=1.0),\n      A.VerticalFlip(p=1.0),\n      A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n      A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n      A.OneOf([\n          A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),\n          A.GridDistortion(p=0.5),\n          A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n      ], p=1.0),\n    ], p=1.0)\n\n    return transform",
      "metadata": {
        "id": "QWHYxU2yWDP9",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:34:52.963557Z",
          "iopub.execute_input": "2022-05-31T00:34:52.966292Z",
          "iopub.status.idle": "2022-05-31T00:34:52.988168Z",
          "shell.execute_reply.started": "2022-05-31T00:34:52.966258Z",
          "shell.execute_reply": "2022-05-31T00:34:52.987412Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "x_train = glob.glob('/kaggle/input/DubaiSat-Semantic-Segmentation/UAE Semantic segmentation dataset/Train/images/*.jpg')\ny_train = glob.glob('/kaggle/input/DubaiSat-Semantic-Segmentation/UAE Semantic segmentation dataset/Train/masks/*.png')\n\nx_test = glob.glob('/kaggle/input/DubaiSat-Semantic-Segmentation/UAE Semantic segmentation dataset/Test/images/*.jpg')\ny_test = glob.glob('/kaggle/input/DubaiSat-Semantic-Segmentation/UAE Semantic segmentation dataset/Test/masks/*.png')\n\nx_train.sort()\ny_train.sort()\n\nx_test.sort()\ny_test.sort()\n\nprint(len(x_train),len(y_train),len(x_test),len(y_test))\n\nx_train, y_train = get_dataset(x_train, y_train, color_dict, 0)\nx_test, y_test = get_dataset(x_test, y_test, color_dict, 1)\n\nprint(x_train.shape ,y_train.shape,x_test.shape,y_test.shape)",
      "metadata": {
        "id": "vqydkfsUWHjC",
        "outputId": "03fe117d-9f76-4c14-8d63-917810c0ff3c",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:34:52.991091Z",
          "iopub.execute_input": "2022-05-31T00:34:52.991487Z",
          "iopub.status.idle": "2022-05-31T00:35:28.809960Z",
          "shell.execute_reply.started": "2022-05-31T00:34:52.991457Z",
          "shell.execute_reply": "2022-05-31T00:35:28.808983Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for i in range(0,2):\n    fig, ax = plt.subplots(nrows=2, ncols=9)\n    fig.tight_layout()\n    fig.set_size_inches(20, 20)\n\n    for j in range(0,9):\n        ax[0][j].imshow(x_train[9*i + j])\n        ax[0][j].axis('off')\n        ax[0][j].set_title('train_img_'+str(i))\n        ax[1][j].imshow(onehot_to_rgb(y_train[9*i + j], color_dict),cmap='gray')\n        ax[1][j].axis('off')\n        ax[1][j].set_title('train_gt')\n\n    fig.subplots_adjust(wspace=0.2,hspace=-0.86)\n    plt.show()",
      "metadata": {
        "id": "HKmmbcQaWMMz",
        "outputId": "3a25fa34-3a70-42f4-e9bb-b8aa8c7bb31b",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:35:28.811314Z",
          "iopub.execute_input": "2022-05-31T00:35:28.811869Z",
          "iopub.status.idle": "2022-05-31T00:35:31.797185Z",
          "shell.execute_reply.started": "2022-05-31T00:35:28.811830Z",
          "shell.execute_reply": "2022-05-31T00:35:31.796491Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "img_width = 512\nimg_height = 512\nimg_channels = 3\n\ninputs = Input((img_height, img_width, img_channels))\n\nc1 = Conv2D(16, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(inputs)\nc1 = Conv2D(16, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = MaxPooling2D((2,2)) (c1)\n\nc2 = Conv2D(32, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = Conv2D(32, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = MaxPooling2D((2,2)) (c2)\n\nc3 = Conv2D(64, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = Conv2D(64, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = MaxPooling2D((2,2)) (c3)\n\nc4 = Conv2D(128, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = Dropout(0.2)(c4)\nc4 = Conv2D(128, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = MaxPooling2D((2,2)) (c4)\n\nc5 = Conv2D(256, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = Dropout(0.4)(c5)\nc5 = Conv2D(256, (3,3), activation = 'relu', kernel_initializer='he_normal', padding='same')(c5)\n\n# Decoder\n\nu6 = Conv2DTranspose(128,(2,2), strides=(2,2), padding = 'same') (c5)\nu6 = concatenate([u6,c4])\nc6 = Conv2D(128,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(u6)\nc6 = Dropout(0.2)(c6)\nc6 = Conv2D(128,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(c6)\n\nu7 = Conv2DTranspose(64,(2,2), strides=(2,2), padding = 'same') (c6)\nu7 = concatenate([u7,c3])\nc7 = Conv2D(64,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(u7)\nc7 = Conv2D(64,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(c7)\n\nu8 = Conv2DTranspose(32,(2,2), strides=(2,2), padding = 'same') (c7)\nu8 = concatenate([u8,c2])\nc8 = Conv2D(32,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(u8)\nc8 = Conv2D(32,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(c8)\n\nu9 = Conv2DTranspose(16,(2,2), strides=(2,2), padding = 'same') (c8)\nu9 = concatenate([u9,c1],axis = 3)\nc9 = Conv2D(16,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(u9)\nc9 = Conv2D(16,(3,3), activation = 'relu', kernel_initializer= 'he_normal', padding ='same')(c9)\n\noutputs = Conv2D(6,(1,1), activation='softmax')(c9)\n\nmodel = Model(inputs=[inputs], outputs = [outputs])\n\nopt = Adam(learning_rate = 0.0001)\nmodel.compile(optimizer=opt, loss= 'categorical_crossentropy', metrics=['accuracy'])\n\nprint(model.summary())",
      "metadata": {
        "id": "F0ldqIokWN5z",
        "outputId": "85012aa8-7fa0-4374-bb82-29d165883672",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:16:08.178285Z",
          "iopub.execute_input": "2022-05-31T02:16:08.178798Z",
          "iopub.status.idle": "2022-05-31T02:16:08.402565Z",
          "shell.execute_reply.started": "2022-05-31T02:16:08.178765Z",
          "shell.execute_reply": "2022-05-31T02:16:08.401721Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model_checkpoint = ModelCheckpoint('/kaggle/working/UAE_Unet.h5', monitor='val_loss', save_best_only=True)\nhistory = model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=100, batch_size=4, verbose=1, callbacks=[model_checkpoint])",
      "metadata": {
        "id": "zludY0P7WTJj",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:16:08.403819Z",
          "iopub.execute_input": "2022-05-31T02:16:08.404739Z",
          "iopub.status.idle": "2022-05-31T02:31:33.219952Z",
          "shell.execute_reply.started": "2022-05-31T02:16:08.404703Z",
          "shell.execute_reply": "2022-05-31T02:31:33.219135Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.plot(history.history['loss'], )\nax.plot(history.history['val_loss'], '--')\nax.legend(['Train Loss', 'Validation Loss'])\nax.set_title('Training/Validation Loss per Epoch')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nplt.show()",
      "metadata": {
        "id": "R24NNG0QWVCD",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:31:33.221908Z",
          "iopub.execute_input": "2022-05-31T02:31:33.222314Z",
          "iopub.status.idle": "2022-05-31T02:31:33.418800Z",
          "shell.execute_reply.started": "2022-05-31T02:31:33.222276Z",
          "shell.execute_reply": "2022-05-31T02:31:33.417943Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.plot(history.history['accuracy'], )\nax.plot(history.history['val_accuracy'], '--')\nax.legend(['Train accuracy', 'Validation accuracy'])\nax.set_title('Training/Validation accuracy per Epoch')\nax.set_xlabel('Epoch')\nax.set_ylabel('accuracy')\nplt.show()",
      "metadata": {
        "id": "ZkTcFUwEWXKL",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:31:33.419988Z",
          "iopub.execute_input": "2022-05-31T02:31:33.421011Z",
          "iopub.status.idle": "2022-05-31T02:31:33.627331Z",
          "shell.execute_reply.started": "2022-05-31T02:31:33.420969Z",
          "shell.execute_reply": "2022-05-31T02:31:33.626590Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model.load_weights('/kaggle/working/UAE_Unet.h5')\ny_pred = model.predict(x_test)",
      "metadata": {
        "id": "AFVSK4ZYWZSg",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:31:33.628439Z",
          "iopub.execute_input": "2022-05-31T02:31:33.628824Z",
          "iopub.status.idle": "2022-05-31T02:31:34.998428Z",
          "shell.execute_reply.started": "2022-05-31T02:31:33.628788Z",
          "shell.execute_reply": "2022-05-31T02:31:34.997598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for i in range(1):\n    fig, ax = plt.subplots(3, 9)\n    fig.tight_layout()\n    fig.set_size_inches(20, 20)\n\n    for j in range(0,9):\n        ax[0][j].imshow(x_test[9*i + j])\n        ax[0][j].axis('off')\n        ax[0][j].set_title('test_img_'+str(i))\n        ax[1][j].imshow(onehot_to_rgb(y_test[9*i + j], color_dict),cmap='gray')\n        ax[1][j].axis('off')\n        ax[1][j].set_title('test_gt')\n        ax[2][j].imshow(onehot_to_rgb(y_pred[9*i + j], color_dict),cmap='gray')\n        ax[2][j].axis('off')\n        ax[2][j].set_title('test_pred')\n\n    fig.subplots_adjust(wspace=0.2,hspace=-0.86)\n    plt.show()",
      "metadata": {
        "id": "rM2fQbMsWacG",
        "outputId": "76589044-8a72-41cf-9a29-cf60b4edb69a",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:31:34.999942Z",
          "iopub.execute_input": "2022-05-31T02:31:35.000302Z",
          "iopub.status.idle": "2022-05-31T02:31:37.971493Z",
          "shell.execute_reply.started": "2022-05-31T02:31:35.000264Z",
          "shell.execute_reply": "2022-05-31T02:31:37.970768Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n\ndef build_inception_resnetv2_unet(input_shape):\n    \"\"\" Input \"\"\"\n    inputs = Input(input_shape)\n\n    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n\n    \"\"\" Encoder \"\"\"\n    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n\n    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n\n    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n\n    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n\n    \"\"\" Bridge \"\"\"\n    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n\n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n    \n    \"\"\" Output \"\"\"\n    dropout = Dropout(0.3)(d4)\n    outputs = Conv2D(6, 1, padding=\"same\", activation=\"softmax\")(dropout)\n\n    model = Model(inputs, outputs, name=\"InceptionResNetV2-UNet\")\n    return model\n\nmodel = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\nmodel.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[\"accuracy\"])\nmodel.summary()",
      "metadata": {
        "id": "lbwg-7TNWcF2",
        "outputId": "e0eb17dc-973f-40ae-9fc5-e538087b5de4",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:35:31.799153Z",
          "iopub.execute_input": "2022-05-31T00:35:31.799664Z",
          "iopub.status.idle": "2022-05-31T00:35:37.459225Z",
          "shell.execute_reply.started": "2022-05-31T00:35:31.799629Z",
          "shell.execute_reply": "2022-05-31T00:35:37.458454Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model_checkpoint = ModelCheckpoint('/kaggle/working/UAE_inception.h5', monitor='val_loss', save_best_only=True)\nhistory = model.fit(x=x_train, y=y_train, validation_data=(x_test,y_test),epochs=100, batch_size=4, verbose=1, callbacks=[model_checkpoint])",
      "metadata": {
        "id": "VFGas7vzWdiT",
        "outputId": "7060fce7-487f-4efb-a630-fc5e4bfdcf65",
        "execution": {
          "iopub.status.busy": "2022-05-31T00:35:37.460526Z",
          "iopub.execute_input": "2022-05-31T00:35:37.461808Z",
          "iopub.status.idle": "2022-05-31T02:15:59.915353Z",
          "shell.execute_reply.started": "2022-05-31T00:35:37.461769Z",
          "shell.execute_reply": "2022-05-31T02:15:59.914562Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.plot(history.history['loss'], )\nax.plot(history.history['val_loss'], '--')\nax.legend(['Train Loss', 'Validation Loss'])\nax.set_title('Training/Validation Loss per Epoch')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nplt.show()",
      "metadata": {
        "id": "BdfzlQipWfaA",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:15:59.917250Z",
          "iopub.execute_input": "2022-05-31T02:15:59.917620Z",
          "iopub.status.idle": "2022-05-31T02:16:00.120346Z",
          "shell.execute_reply.started": "2022-05-31T02:15:59.917580Z",
          "shell.execute_reply": "2022-05-31T02:16:00.119609Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.plot(history.history['accuracy'], )\nax.plot(history.history['val_accuracy'], '--')\nax.legend(['Train accuracy', 'Validation accuracy'])\nax.set_title('Training/Validation accuracy per Epoch')\nax.set_xlabel('Epoch')\nax.set_ylabel('accuracy')\nplt.show()",
      "metadata": {
        "id": "2wMjSnluWg0t",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:16:00.121638Z",
          "iopub.execute_input": "2022-05-31T02:16:00.122143Z",
          "iopub.status.idle": "2022-05-31T02:16:00.312346Z",
          "shell.execute_reply.started": "2022-05-31T02:16:00.122107Z",
          "shell.execute_reply": "2022-05-31T02:16:00.311608Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model.load_weights('/kaggle/working/UAE_inception.h5')\ny_pred = model.predict(x_test)",
      "metadata": {
        "id": "70kw37J0WiOo",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:16:00.313602Z",
          "iopub.execute_input": "2022-05-31T02:16:00.314814Z",
          "iopub.status.idle": "2022-05-31T02:16:05.886433Z",
          "shell.execute_reply.started": "2022-05-31T02:16:00.314772Z",
          "shell.execute_reply": "2022-05-31T02:16:05.885598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for i in range(1):\n    fig, ax = plt.subplots(nrows=3, ncols=9)\n    fig.tight_layout()\n    fig.set_size_inches(20, 20)\n\n    for j in range(0,9):\n        ax[0][j].imshow(x_test[9*i + j])\n        ax[0][j].axis('off')\n        ax[0][j].set_title('test_img_'+str(i))\n        ax[1][j].imshow(onehot_to_rgb(y_test[9*i + j], color_dict),cmap='gray')\n        ax[1][j].axis('off')\n        ax[1][j].set_title('test_gt')\n        ax[2][j].imshow(onehot_to_rgb(y_pred[9*i + j], color_dict),cmap='gray')\n        ax[2][j].axis('off')\n        ax[2][j].set_title('test_pred')\n\n    fig.subplots_adjust(wspace=0.2,hspace=-0.86)\n    plt.show()",
      "metadata": {
        "id": "6C12Q1XsWjYJ",
        "execution": {
          "iopub.status.busy": "2022-05-31T02:16:05.889222Z",
          "iopub.execute_input": "2022-05-31T02:16:05.889634Z",
          "iopub.status.idle": "2022-05-31T02:16:08.176957Z",
          "shell.execute_reply.started": "2022-05-31T02:16:05.889596Z",
          "shell.execute_reply": "2022-05-31T02:16:08.176054Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
